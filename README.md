ğŸ“š RAG with Local LLM (Ollama)
This project implements a Retrieval-Augmented Generation (RAG) pipeline using a local LLM powered by Ollama. It allows you to upload PDFs, extract and vectorize content, and query information using natural language, all locally for maximum privacy and offline capability.

âœ… Features
Local LLM integration with Ollama (privacy-first, no cloud API).

RAG architecture for improved context-based answers.

PDF ingestion and processing (extract text, chunk, vectorize).

Embeddings using Ollama.

Vector database for semantic search.

Streamlit interface for easy interaction.

ğŸ› ï¸ Requirements
Python 3.9+

Ollama installed (Download here)

Dependencies (install via requirements.txt):

bash
Copy
Edit
pip install -r requirements.txt
ğŸ“‚ Project Structure
graphql
Copy
Edit
.
â”œâ”€â”€ local_llm/
â”‚   â”œâ”€â”€ crawl.py           # Upload, extract, and vectorize PDFs (Ollama embeddings)
â”‚   â”œâ”€â”€ model.py           # LLM integration, model setup, RAG templates
â”‚   â”œâ”€â”€ pdfinjest.py       # Main script: Streamlit app, PDF ingestion, retrieval
â”‚   â”œâ”€â”€ vectordatabase.py  # Vector database management functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
ğŸš€ Getting Started
1. Install Ollama
Download and install Ollama for your platform:
Ollama Installation Guide

Verify:

bash
Copy
Edit
ollama --version
2. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Pull Your Model
For example, to use LLaMA 2:

bash
Copy
Edit
ollama pull llama2
Other models available: mistral, gemma, etc. (see Ollama Library).

4. Run the Streamlit App
bash
Copy
Edit
streamlit run local_llm/pdfinjest.py
This will:

Start the UI for uploading PDFs.

Ingest, process, and store embeddings in the vector database.

Enable querying the uploaded documents with local LLM.

ğŸ” Workflow
Upload PDF â†’ crawl.py extracts text, chunks it, and creates embeddings using Ollama.

Store in Vector DB â†’ vectordatabase.py handles insertion and retrieval.

Query with RAG â†’ model.py integrates the LLM and uses retrieved context for responses.

Interact via Streamlit â†’ pdfinjest.py provides a user-friendly interface.

ğŸ–¥ï¸ Example Usage
Upload and Process PDF
bash
Copy
Edit
python local_llm/crawl.py --file sample.pdf
Query Through UI
Launch Streamlit and ask:

sql
Copy
Edit
What are the key points discussed in the PDF?
âœ… Advantages
100% local execution (privacy-first).

Offline capable.

Scalable to multiple PDFs and models.

âš ï¸ Limitations
Requires sufficient RAM & storage for large models.

Performance depends on model size and hardware.

ğŸ“œ License
MIT License.

